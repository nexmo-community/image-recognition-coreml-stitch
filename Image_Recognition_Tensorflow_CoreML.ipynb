{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Recognition - Tensorflow / CoreML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D_tG7dsh0QsF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ]
    },
    {
      "metadata": {
        "id": "e-rxun5qXCvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the following, we are going to build a basic image reconigition model to be used in a Stitch IOS app\n",
        "We will use the  [CIFAIR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset for training.\n",
        "\n",
        "This dataset contains images in 10 classes, with 6000 images per class.\n",
        "\n",
        "Its a well used dataset for Machine Learning, and it will be a good start for our project.\n",
        "\n",
        "Since the dataset is fairly small, we can train the model locally. \n",
        "\n",
        "Note, this notebook is currently being hosted on Google Colab. But you should be able to download this notebook, create a virtual enviorment using virtualenv or anaconda.\n",
        "\n",
        "You will need to install the following packages:\n",
        "\n",
        "\n",
        "```\n",
        "tensorflow\n",
        "matplotlib\n",
        "numpy\n",
        "coremltools\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "X5QaMahPWAnG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Running this notenook"
      ]
    },
    {
      "metadata": {
        "id": "kSU76Me2WIPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook is currently being hosted on [Google Colab](https://colab.research.google.com). Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud\n",
        "\n",
        "You can view the notebook and run the code [here](https://colab.research.google.com/github/tbass134/colab_notebooks/blob/master/Image_Recognition_Tensorflow_CoreML.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "LnKvGq42Ybs2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building out the Model"
      ]
    },
    {
      "metadata": {
        "id": "xuZFgrFjXTOp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First thing we need to do is to import our packages\n"
      ]
    },
    {
      "metadata": {
        "id": "31G4vI2XXYnC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWzsH7Gkdi8N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice, were using Keras as a backend for Tensorflow.\n",
        "Keras is a great framework that allows you to build models easier, without having to use the more verbose methods in Tensorflow. More information about Keras is [here](https://www.tensorflow.org/guide/keras): "
      ]
    },
    {
      "metadata": {
        "id": "DXYThnXaaYkW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll load the CIFAR dataset. Using Keras, we're able to download the dataset very easily.\n",
        "\n",
        "We split the dataset into 2 groups, one for training `(x_train, y_train)`, the other for testing `(x_test, y_test)`. \n",
        "\n",
        "Splitting the data set allows the model to learn from the training set, then, when we test the model, we want to see how well it learned, by using the test set. This will give us our accuracy, or, how we'll the model did."
      ]
    },
    {
      "metadata": {
        "id": "H6hioSlLX-KU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "f138575f-f24e-4a10-95ab-fb19830402c4"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "\n",
        "print('y_train shape', y_train.shape)\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "print('x_test shape', x_test.shape)\n",
        "print(y_test.shape[0], 'test samples')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 46s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "y_train shape (50000, 1)\n",
            "10000 test samples\n",
            "x_test shape (10000, 32, 32, 3)\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsFtm_ODPzex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll declair some constants\n",
        "`batch_size` is the number of samples that going to be propagated through the network. \n",
        "\n",
        "`epochs` are how many times we train on the full dataset\n",
        "\n",
        "`class_names` is a list of all the possible lables in the CIFAR-10 dataset.\n",
        "We'll use this later, when coverting our model into CoreML"
      ]
    },
    {
      "metadata": {
        "id": "aGptaSCu8Qj2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare variables\n",
        "batch_size = 32 \n",
        "# 32 examples in a mini-batch, smaller batch size means more updates in one epoch\n",
        "epochs = 100\n",
        "class_names = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PIQ3hprb7_fV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images(x, y, number_of_images=5):\n",
        "  fig, axes1 = plt.subplots(number_of_images,number_of_images,figsize=(10,10))\n",
        "  for j in range(number_of_images):\n",
        "      for k in range(number_of_images):\n",
        "          i = np.random.choice(range(len(x)))\n",
        "          title = class_names[y[i:i+1][0][0]]\n",
        "          axes1[j][k].title.set_text(title)\n",
        "          axes1[j][k].set_axis_off()\n",
        "          axes1[j][k].imshow(x[i:i+1][0])        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mL4y0R5a1lQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, lets have a look at the images.\n",
        "We have a function that plots a series of images and their corresponding label"
      ]
    },
    {
      "metadata": {
        "id": "OSqihPKPDmaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "ab4e90b9-ed73-4836-8485-bf612696c161"
      },
      "cell_type": "code",
      "source": [
        "plot_images(x_train, y_train, number_of_images=2)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAJNCAYAAADK04ocAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAIMCABAAAEGJAAAAACDEgAAACB/wNwnLepgCWwWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa809794208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ldpPL_D3alSR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we'll setup a simple model. You don't need to understand everthing going on here. But what we are basiclly doing is creating a deep neural network using a few convolutions, dropout, and max pooling\n",
        "\n",
        "At the end, we'll flatten the network and use Relu, followed by a Softmax. \n",
        "This will give us a vector (1-dimenstion matrix), filled with mostly 0's\n",
        "\n",
        "It will look like this\n",
        "\n",
        "```\n",
        "[0,0,0,0,0,0,1,0,0,0]\n",
        "```\n",
        "\n",
        "This vector corrsponds to the given label from the image\n",
        "So in this example, the `1` in the 7'th place would be a frog, since 'frog' is at the 7th place in `class_names` list\n",
        "\n",
        "---\n",
        "\n",
        "The following shows the entire network."
      ]
    },
    {
      "metadata": {
        "id": "mGv6JeG7YmZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fi-HoS6X3Baa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's it! "
      ]
    },
    {
      "metadata": {
        "id": "WBR89DlwZURX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training The Model"
      ]
    },
    {
      "metadata": {
        "id": "Usmc-ST7nX0D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First we compile the model to get its loss.\n",
        "The loss is a measure of how well the model did during testing. A high loss, means that the model did poorly. \n",
        "\n",
        "We'll use the Adam Optimizer to calucate the loss. You can learn more about Optimizers in keras [here](https://keras.io/optimizers/)\n",
        "\n",
        "Then we'll call `.fit` which will train the model for 100 epochs. This means that the full training dataset will trained 100 times,\n",
        "The `batch_size` of 32 is the number samples that going to be propagated through the network\n",
        "\n",
        "We then see how well it did after every echo using `model.evaluate`\n",
        "It gives us a score for the model (higher numbers are better) and the loss (lower numbers are better)\n",
        "\n",
        "---\n",
        "\n",
        "Note, this took about 15 minutes  running on Colab. If you just wants to see the results quicker, just set the `epochs` parameter to 1 or 2. Its accuracy wont be as good, however.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8V6jjkCyaxQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3505
        },
        "outputId": "fef46107-872e-4278-de8f-1f9c3bb93aed"
      },
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.0001, decay=1e-6),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train / 255.0, tf.keras.utils.to_categorical(y_train),\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test / 255.0, tf.keras.utils.to_categorical(y_test))\n",
        "          )\n",
        "\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(x_test / 255.0, tf.keras.utils.to_categorical(y_test))\n",
        "\n",
        "print('Loss: %.3f' % scores[0])\n",
        "print('Accuracy: %.3f' % scores[1])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 32s 641us/step - loss: 1.6316 - acc: 0.3976 - val_loss: 1.4677 - val_acc: 0.4692\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 32s 632us/step - loss: 1.4277 - acc: 0.4804 - val_loss: 1.2824 - val_acc: 0.5430\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 31s 619us/step - loss: 1.2916 - acc: 0.5360 - val_loss: 1.1822 - val_acc: 0.5742\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 31s 620us/step - loss: 1.1982 - acc: 0.5716 - val_loss: 1.0952 - val_acc: 0.6105\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 31s 619us/step - loss: 1.1240 - acc: 0.6007 - val_loss: 1.0197 - val_acc: 0.6409\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 31s 621us/step - loss: 1.0665 - acc: 0.6242 - val_loss: 0.9897 - val_acc: 0.6535\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 31s 620us/step - loss: 1.0082 - acc: 0.6477 - val_loss: 0.9433 - val_acc: 0.6662\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 31s 617us/step - loss: 0.9656 - acc: 0.6605 - val_loss: 0.8940 - val_acc: 0.6801\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 31s 617us/step - loss: 0.9181 - acc: 0.6779 - val_loss: 0.8740 - val_acc: 0.6936\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.8834 - acc: 0.6910 - val_loss: 0.8412 - val_acc: 0.7033\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.8501 - acc: 0.7038 - val_loss: 0.8080 - val_acc: 0.7217\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.8191 - acc: 0.7129 - val_loss: 0.8048 - val_acc: 0.7182\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 31s 620us/step - loss: 0.7934 - acc: 0.7236 - val_loss: 0.7577 - val_acc: 0.7328\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 31s 620us/step - loss: 0.7647 - acc: 0.7327 - val_loss: 0.7651 - val_acc: 0.7302\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 31s 616us/step - loss: 0.7389 - acc: 0.7412 - val_loss: 0.7262 - val_acc: 0.7489\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.7144 - acc: 0.7485 - val_loss: 0.7169 - val_acc: 0.7538\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.6947 - acc: 0.7556 - val_loss: 0.7121 - val_acc: 0.7544\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 31s 619us/step - loss: 0.6688 - acc: 0.7661 - val_loss: 0.6829 - val_acc: 0.7591\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 31s 619us/step - loss: 0.6485 - acc: 0.7732 - val_loss: 0.6828 - val_acc: 0.7628\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.6322 - acc: 0.7803 - val_loss: 0.6561 - val_acc: 0.7716\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 31s 621us/step - loss: 0.6112 - acc: 0.7868 - val_loss: 0.6497 - val_acc: 0.7760\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 31s 624us/step - loss: 0.5918 - acc: 0.7931 - val_loss: 0.6435 - val_acc: 0.7774\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 31s 630us/step - loss: 0.5793 - acc: 0.7972 - val_loss: 0.6343 - val_acc: 0.7797\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 31s 623us/step - loss: 0.5669 - acc: 0.8033 - val_loss: 0.6323 - val_acc: 0.7813\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 32s 633us/step - loss: 0.5476 - acc: 0.8088 - val_loss: 0.6124 - val_acc: 0.7866\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.5373 - acc: 0.8123 - val_loss: 0.6451 - val_acc: 0.7791\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 32s 636us/step - loss: 0.5233 - acc: 0.8159 - val_loss: 0.6052 - val_acc: 0.7929\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 32s 635us/step - loss: 0.5082 - acc: 0.8225 - val_loss: 0.6162 - val_acc: 0.7913\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.4948 - acc: 0.8244 - val_loss: 0.5903 - val_acc: 0.7959\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 32s 636us/step - loss: 0.4823 - acc: 0.8305 - val_loss: 0.6053 - val_acc: 0.7916\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 32s 635us/step - loss: 0.4713 - acc: 0.8336 - val_loss: 0.6254 - val_acc: 0.7893\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 32s 636us/step - loss: 0.4595 - acc: 0.8399 - val_loss: 0.5850 - val_acc: 0.7977\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.4463 - acc: 0.8441 - val_loss: 0.5986 - val_acc: 0.7964\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 32s 646us/step - loss: 0.4356 - acc: 0.8459 - val_loss: 0.6012 - val_acc: 0.7979\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 31s 621us/step - loss: 0.4279 - acc: 0.8495 - val_loss: 0.5935 - val_acc: 0.7971\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 32s 634us/step - loss: 0.4172 - acc: 0.8524 - val_loss: 0.5789 - val_acc: 0.8014\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 31s 627us/step - loss: 0.4026 - acc: 0.8565 - val_loss: 0.5828 - val_acc: 0.8062\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 31s 623us/step - loss: 0.3987 - acc: 0.8599 - val_loss: 0.5774 - val_acc: 0.8079\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 31s 625us/step - loss: 0.3893 - acc: 0.8620 - val_loss: 0.5821 - val_acc: 0.8027\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 31s 617us/step - loss: 0.3786 - acc: 0.8656 - val_loss: 0.5906 - val_acc: 0.8018\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 31s 622us/step - loss: 0.3675 - acc: 0.8697 - val_loss: 0.5928 - val_acc: 0.8045\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 31s 625us/step - loss: 0.3610 - acc: 0.8729 - val_loss: 0.5804 - val_acc: 0.8049\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 31s 623us/step - loss: 0.3519 - acc: 0.8746 - val_loss: 0.5728 - val_acc: 0.8084\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 31s 618us/step - loss: 0.3460 - acc: 0.8755 - val_loss: 0.5778 - val_acc: 0.8095\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 31s 623us/step - loss: 0.3337 - acc: 0.8825 - val_loss: 0.5768 - val_acc: 0.8061\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 31s 621us/step - loss: 0.3298 - acc: 0.8828 - val_loss: 0.5888 - val_acc: 0.8070\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 31s 611us/step - loss: 0.3220 - acc: 0.8845 - val_loss: 0.5856 - val_acc: 0.8094\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 31s 611us/step - loss: 0.3156 - acc: 0.8874 - val_loss: 0.6057 - val_acc: 0.8045\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 30s 609us/step - loss: 0.3064 - acc: 0.8905 - val_loss: 0.5850 - val_acc: 0.8098\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 30s 608us/step - loss: 0.2987 - acc: 0.8938 - val_loss: 0.5837 - val_acc: 0.8121\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 31s 611us/step - loss: 0.2960 - acc: 0.8943 - val_loss: 0.5770 - val_acc: 0.8124\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 31s 614us/step - loss: 0.2883 - acc: 0.8972 - val_loss: 0.5878 - val_acc: 0.8114\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 30s 609us/step - loss: 0.2852 - acc: 0.8979 - val_loss: 0.5914 - val_acc: 0.8119\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.2769 - acc: 0.9020 - val_loss: 0.5896 - val_acc: 0.8125\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 30s 608us/step - loss: 0.2718 - acc: 0.9024 - val_loss: 0.5985 - val_acc: 0.8092\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 30s 605us/step - loss: 0.2650 - acc: 0.9059 - val_loss: 0.5931 - val_acc: 0.8142\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 30s 602us/step - loss: 0.2627 - acc: 0.9063 - val_loss: 0.6015 - val_acc: 0.8119\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 30s 604us/step - loss: 0.2561 - acc: 0.9081 - val_loss: 0.5958 - val_acc: 0.8108\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 30s 604us/step - loss: 0.2523 - acc: 0.9098 - val_loss: 0.5948 - val_acc: 0.8131\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 31s 611us/step - loss: 0.2494 - acc: 0.9105 - val_loss: 0.5940 - val_acc: 0.8126\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 30s 603us/step - loss: 0.2401 - acc: 0.9140 - val_loss: 0.6047 - val_acc: 0.8109\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.2356 - acc: 0.9144 - val_loss: 0.6181 - val_acc: 0.8100\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 31s 611us/step - loss: 0.2282 - acc: 0.9186 - val_loss: 0.6155 - val_acc: 0.8145\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 30s 609us/step - loss: 0.2278 - acc: 0.9176 - val_loss: 0.6290 - val_acc: 0.8106\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 30s 610us/step - loss: 0.2225 - acc: 0.9203 - val_loss: 0.6242 - val_acc: 0.8111\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 31s 610us/step - loss: 0.2189 - acc: 0.9211 - val_loss: 0.6167 - val_acc: 0.8165\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 31s 616us/step - loss: 0.2162 - acc: 0.9236 - val_loss: 0.6090 - val_acc: 0.8155\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 31s 617us/step - loss: 0.2088 - acc: 0.9251 - val_loss: 0.6200 - val_acc: 0.8126\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 31s 617us/step - loss: 0.2112 - acc: 0.9240 - val_loss: 0.6220 - val_acc: 0.8142\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.2078 - acc: 0.9248 - val_loss: 0.6490 - val_acc: 0.8149\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 30s 608us/step - loss: 0.2005 - acc: 0.9288 - val_loss: 0.6174 - val_acc: 0.8149\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 30s 605us/step - loss: 0.1976 - acc: 0.9292 - val_loss: 0.6086 - val_acc: 0.8179\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 30s 609us/step - loss: 0.1983 - acc: 0.9287 - val_loss: 0.6373 - val_acc: 0.8134\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.1916 - acc: 0.9311 - val_loss: 0.6311 - val_acc: 0.8125\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 30s 604us/step - loss: 0.1860 - acc: 0.9351 - val_loss: 0.6386 - val_acc: 0.8186\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.1847 - acc: 0.9347 - val_loss: 0.6356 - val_acc: 0.8149\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 30s 607us/step - loss: 0.1812 - acc: 0.9353 - val_loss: 0.6478 - val_acc: 0.8135\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 30s 607us/step - loss: 0.1815 - acc: 0.9356 - val_loss: 0.6439 - val_acc: 0.8123\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 30s 610us/step - loss: 0.1814 - acc: 0.9353 - val_loss: 0.6480 - val_acc: 0.8143\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.1739 - acc: 0.9385 - val_loss: 0.6543 - val_acc: 0.8188\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 30s 605us/step - loss: 0.1711 - acc: 0.9402 - val_loss: 0.6456 - val_acc: 0.8149\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 30s 609us/step - loss: 0.1709 - acc: 0.9400 - val_loss: 0.6496 - val_acc: 0.8139\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 30s 607us/step - loss: 0.1655 - acc: 0.9401 - val_loss: 0.6647 - val_acc: 0.8162\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 30s 608us/step - loss: 0.1643 - acc: 0.9407 - val_loss: 0.6673 - val_acc: 0.8138\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 30s 607us/step - loss: 0.1616 - acc: 0.9412 - val_loss: 0.6435 - val_acc: 0.8162\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 30s 605us/step - loss: 0.1604 - acc: 0.9441 - val_loss: 0.6633 - val_acc: 0.8187\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 30s 603us/step - loss: 0.1583 - acc: 0.9426 - val_loss: 0.6902 - val_acc: 0.8138\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 30s 596us/step - loss: 0.1564 - acc: 0.9427 - val_loss: 0.6433 - val_acc: 0.8180\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 30s 600us/step - loss: 0.1540 - acc: 0.9455 - val_loss: 0.6748 - val_acc: 0.8146\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 30s 600us/step - loss: 0.1538 - acc: 0.9445 - val_loss: 0.6791 - val_acc: 0.8136\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 30s 601us/step - loss: 0.1494 - acc: 0.9469 - val_loss: 0.6756 - val_acc: 0.8129\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 30s 599us/step - loss: 0.1512 - acc: 0.9471 - val_loss: 0.6760 - val_acc: 0.8170\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 30s 598us/step - loss: 0.1452 - acc: 0.9482 - val_loss: 0.6747 - val_acc: 0.8174\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 30s 600us/step - loss: 0.1495 - acc: 0.9470 - val_loss: 0.6690 - val_acc: 0.8171\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 30s 599us/step - loss: 0.1417 - acc: 0.9497 - val_loss: 0.6917 - val_acc: 0.8200\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 30s 599us/step - loss: 0.1427 - acc: 0.9491 - val_loss: 0.6778 - val_acc: 0.8156\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 30s 602us/step - loss: 0.1355 - acc: 0.9514 - val_loss: 0.6887 - val_acc: 0.8168\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 30s 605us/step - loss: 0.1364 - acc: 0.9515 - val_loss: 0.6810 - val_acc: 0.8174\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 30s 606us/step - loss: 0.1368 - acc: 0.9520 - val_loss: 0.6792 - val_acc: 0.8188\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 30s 603us/step - loss: 0.1378 - acc: 0.9518 - val_loss: 0.7136 - val_acc: 0.8116\n",
            "10000/10000 [==============================] - 2s 151us/step\n",
            "Loss: 0.714\n",
            "Accuracy: 0.812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "30lMTf95btqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our final accuracy was 81%, and our loss was 0.7, which is pretty good,\n",
        "\n",
        "To reiterate, accuracy is how well the model was able to classify each image, whille loss indicate how bad the model's predictions were.\n",
        "\n",
        "For more information, check out this defination of loss and accuracy on [Google\n",
        "s Machine Learning crash course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
      ]
    },
    {
      "metadata": {
        "id": "ulDRlAQQ4MKN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Coverting the model to CoreML"
      ]
    },
    {
      "metadata": {
        "id": "WX6LUzj2KpWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After we have trained the model, we can save the model, then convert into the coreML format.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "First, we need to save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "fxX-HW8OKu3q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('cifar-model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-H2PJ_FDK1SG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll use CoremlTools to convert the model into a format that our Stitch app can use. \n",
        "\n",
        "First, we'll install coremltools, which will help us convert the model into something our IOS app can use.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Note, we use `pip` to install it, since were running on Collab. to run in locally, just add this package into your virtual enviroment\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x8aN28emb73Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "f8be9f64-a5c0-4aa7-ba65-10c5a15db2a5"
      },
      "cell_type": "code",
      "source": [
        "!pip install coremltools"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting coremltools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/ab/b4dea5ab2503f3e601052958985153cd41bd4f9a336fb74f6789151d976e/coremltools-0.8-py3.5-none-manylinux1_x86_64.whl (2.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.5MB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from coremltools) (3.6.0)\n",
            "Collecting six==1.10.0 (from coremltools)\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b44607ab535be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from coremltools) (1.14.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->coremltools) (39.1.0)\n",
            "Installing collected packages: six, coremltools\n",
            "  Found existing installation: six 1.11.0\n",
            "    Uninstalling six-1.11.0:\n",
            "      Successfully uninstalled six-1.11.0\n",
            "Successfully installed coremltools-0.8 six-1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "twPDKR6gcilU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next, we'll  convert the saved model into CoreML. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Since we have used Keras to train our model, Its really easy to convert to CoreML. However, this varies based on how you built your model. CoreML tools has other functions to use for other machine learning packages including Tensorflow and Scikit Learn.  See the [coremltools repo](https://github.com/apple/coremltools) for more info.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2W71YwoxOts5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e7e9bd88-cf92-4e21-a43a-f428ceaf5562"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model  \n",
        "import coremltools\n",
        "\n",
        "model = load_model('cifar-model.h5')\n",
        "coreml_model = coremltools.converters.keras.convert(model,\n",
        "\tinput_names=\"image\",\n",
        "\timage_input_names=\"image\",\n",
        "\timage_scale=1/255.0,\n",
        "\tclass_labels=class_names)\n",
        "\n",
        "coreml_model.save('CIFAR.mlmodel')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 : conv2d_input, <keras.engine.topology.InputLayer object at 0x7fa7c829fac8>\n",
            "1 : conv2d, <keras.layers.convolutional.Conv2D object at 0x7fa7c829f358>\n",
            "2 : conv2d__activation__, <keras.layers.core.Activation object at 0x7fa7c75bf198>\n",
            "3 : conv2d_1, <keras.layers.convolutional.Conv2D object at 0x7fa7c80e40b8>\n",
            "4 : conv2d_1__activation__, <keras.layers.core.Activation object at 0x7fa7c75bf438>\n",
            "5 : max_pooling2d, <keras.layers.pooling.MaxPooling2D object at 0x7fa7c80e4550>\n",
            "6 : conv2d_2, <keras.layers.convolutional.Conv2D object at 0x7fa7c77434a8>\n",
            "7 : conv2d_2__activation__, <keras.layers.core.Activation object at 0x7fa7c73f9240>\n",
            "8 : max_pooling2d_1, <keras.layers.pooling.MaxPooling2D object at 0x7fa7c7743f28>\n",
            "9 : conv2d_3, <keras.layers.convolutional.Conv2D object at 0x7fa7c87ad1d0>\n",
            "10 : conv2d_3__activation__, <keras.layers.core.Activation object at 0x7fa7c7262dd8>\n",
            "11 : max_pooling2d_2, <keras.layers.pooling.MaxPooling2D object at 0x7fa7c7743f60>\n",
            "12 : flatten, <keras.layers.core.Flatten object at 0x7fa7c76fac50>\n",
            "13 : dense, <keras.layers.core.Dense object at 0x7fa7c76b42b0>\n",
            "14 : dense__activation__, <keras.layers.core.Activation object at 0x7fa7c71ff390>\n",
            "15 : dense_1, <keras.layers.core.Dense object at 0x7fa7c7670358>\n",
            "16 : dense_1__activation__, <keras.layers.core.Activation object at 0x7fa7c71ff898>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7xOpPnaNcQpX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output above shows all the layers inside the model. These directly correlate to how we created the model in this [cell](https://colab.research.google.com/drive/1KJ2PPzzbUjAo8SASpDcuiikBRNFl1VGV#scrollTo=mGv6JeG7YmZL&line=3&uniqifier=1)\n",
        "\n",
        "Take a look at the parameters for the `convert` function\n",
        "here, we'll set the input to be an `image` for both `input_names`  and `image_input_names` parameters. \n",
        "\n",
        "This will help the coreML model know what type of input it is expecting, which is an image.\n",
        "\n",
        "Then scale the images down in `image_scale` parameter.\n",
        "\n",
        "Next, we set the `class_labels`  to `class_names` list. \n",
        "When we use this model in Xcode, the result will be a `String`, corresponding the the predicted label of the image.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HCq0x50-cwvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can have a look at the CoreML model."
      ]
    },
    {
      "metadata": {
        "id": "NeeM4z1FO2aX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "7c46a945-a1c8-49c8-b1e9-3144f81c7b3b"
      },
      "cell_type": "code",
      "source": [
        "coreml_model"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "input {\n",
              "  name: \"image\"\n",
              "  type {\n",
              "    imageType {\n",
              "      width: 32\n",
              "      height: 32\n",
              "      colorSpace: RGB\n",
              "    }\n",
              "  }\n",
              "}\n",
              "output {\n",
              "  name: \"output1\"\n",
              "  type {\n",
              "    dictionaryType {\n",
              "      stringKeyType {\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "output {\n",
              "  name: \"classLabel\"\n",
              "  type {\n",
              "    stringType {\n",
              "    }\n",
              "  }\n",
              "}\n",
              "predictedFeatureName: \"classLabel\"\n",
              "predictedProbabilitiesName: \"output1\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "24GcMs_H5nSv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that our `input` is a 32x32 image, and our output is a String, called `classLabel`\n",
        "\n",
        "Next, we save the mlmodel locally.\n",
        "If you are not using Google Collab, just use \n",
        "\n",
        "` %%writefile \"CIFAR.mlmodel\"`"
      ]
    },
    {
      "metadata": {
        "id": "suCKlGU9c2SD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('CIFAR.mlmodel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TrvJwoNxz2zf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Incorporating the model into our Stitch app"
      ]
    },
    {
      "metadata": {
        "id": "OvjRUM-Sz9QT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once our model is saved, we can now import it into our app. \n",
        "To do this, just drag the model that was just saved into xcode.\n",
        "\n",
        "Make sure the model is included in the target by verifing that \n",
        "`Target Membership` is selected"
      ]
    },
    {
      "metadata": {
        "id": "pqdRN3k18NW6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll write the code that will use the model. "
      ]
    },
    {
      "metadata": {
        "id": "xmzCaPKZDVFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In our Stitch Demo Application, Users' are able to upload a photo into a existing Conversation.\n",
        "\n",
        "For this sample, we use try to predict the photo that the user uploaded.\n",
        "\n",
        "Below is the Swift code inside of our \n",
        "\n",
        "```\n",
        "tableView(_ tableView: UITableView, cellForRowAt indexPath: IndexPath)\n",
        "```\n",
        "function.\n",
        "which is located [here](https://github.com/Nexmo/Stitch-Sample-App/blob/ea6e5af6dcca15a3917657079e9d67decdb904c4/Stitch-Demo/ChatTableViewController.swift#L255)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Note, we instantiate the model using \n",
        " \n",
        "```\n",
        "let model = CIFAR()\n",
        "```\n",
        "\n",
        "This is go in the `ViewDidLoad` method in our class\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Zryev3Zme47A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, lets add the code."
      ]
    },
    {
      "metadata": {
        "id": "dVjhndIQEUri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "let imageEvent = (event as! ImageEvent)\n",
        "guard let imagePath = imageEvent.path(of: IPS.ImageType.thumbnail), let image = UIImage(contentsOfFile: imagePath) else {\n",
        "    break\n",
        "}\n",
        "\n",
        "cell.imageView?.image = image\n",
        "\n",
        "#convert the image to a pixelBuffer\n",
        "#using https://github.com/hollance/CoreMLHelpers.git\n",
        "if let pixelBuffer = image.pixelBuffer(width: 32, height: 32) {\n",
        "    #initalize the model with the pixelBuffer\n",
        "    let input = CIFARInput(image: pixelBuffer)\n",
        "    \n",
        "    #perform the prediction\n",
        "    if let output = try? model.prediction(input: input)  {\n",
        "        #outut returns `classLabel` which is the label of the photo\n",
        "        cell.textLabel?.text = (imageEvent.from?.name)! + \" uploaded a photo of a \\(output.classLabel)\"\n",
        "    }\n",
        "    else {\n",
        "        #we could not get a prediction, just use generic text\n",
        "        cell.textLabel?.text = (imageEvent.from?.name)! + \" uploaded a photo\"\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWL94P0iEZAw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we check if the Conversation Event is an `ImageEvent` \n",
        "If so, we load the thumbnail and set cell's `imageView` to that image\n",
        "\n",
        "Then, we take the image, convert it to a `PixelBuffer`, at a size of 32x32 then, feed it into the model.\n",
        "The reason why we have to resample the image is because the model is trained on images of 32x32, so if we don't resize the images, the model won't be able to give a prediction (We'll see an error in xcode saying that the image size is incorrect)\n",
        "\n",
        "The model will then return a classLabel. This will be the name of the image that the model predicted, which could be one of the following lables\n",
        "\"airplane\", \"automobile\", \"bird\", \"cat\",\" deer\", \"dog\", \"frog\", \"horse\", \"ship\" or \"truck\""
      ]
    },
    {
      "metadata": {
        "id": "J-XCbGg8Kqhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Where do we go from here"
      ]
    },
    {
      "metadata": {
        "id": "WWC6-742KuTv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After looking at our predictions, we can tell that the model will only be able to reconized only 10 labels. \n",
        "\n",
        "This is good for a demo, but not for a production example.\n",
        "\n",
        "In a future post, we'll look at building an image reconition model with more data. \n",
        "\n",
        "We'll look into the popular [ImageNet database](http://www.image-net.org),  which contains 14,197,122 labeled images.\n",
        "Its 150gb download, so, we'll look at how to download it, train, and intergrate into our Stitch demo app."
      ]
    }
  ]
}